{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          64000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 164,106\n",
      "Trainable params: 164,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "# Add an Embedding layer expecting input vocab of size 1000, and\n",
    "# output embedding dimension of size 64.\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dense layer with 10 units.\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is the unit parameter for class LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 30, 3)]           0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               [(None, 30, 64), (None, 6 17408     \n",
      "=================================================================\n",
      "Total params: 17,408\n",
      "Trainable params: 17,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# what is the unit parameter for class LSTM?\n",
    "# see https://zhuanlan.zhihu.com/p/58854907\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "Tx = 30\n",
    "n_x = 3\n",
    "n_s = 64\n",
    "X = Input(shape=(Tx, n_x))   \n",
    "s, a, c = LSTM(n_s, return_sequences=True, return_state=True)(X)    \n",
    "model_LSTM = Model(inputs=X, outputs=[s, a, c])\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 30, 128)           67584     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30, 10)            1290      \n",
      "=================================================================\n",
      "Total params: 68,874\n",
      "Trainable params: 68,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "Tx = 30\n",
    "n_x = 3\n",
    "n_s = 64\n",
    "\n",
    "model.add(Input(shape=(Tx, n_x)))\n",
    "\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(128, return_sequences=True, return_state=False))\n",
    "\n",
    "# Add a Dense layer with 10 units.\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 30, 3)]           0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 30, 128)           67584     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 30, 10)            1290      \n",
      "=================================================================\n",
      "Total params: 68,874\n",
      "Trainable params: 68,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "n_x = 3\n",
    "n_s = 64*2\n",
    "X = Input(shape=(Tx, n_x))   \n",
    "s = LSTM(n_s, return_sequences=True, return_state=False)(X)    \n",
    "d = Dense(10)(s)\n",
    "model_LSTM = Model(inputs=X, outputs=[d])\n",
    "model_LSTM.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic: LSTM return_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (32, 10, 8)\n",
      "output: (32, 4)\n",
      "whole_sequence_output: (32, 10, 4), final_memory_state: (32, 4), final_carry_state: (32, 4)\n"
     ]
    }
   ],
   "source": [
    "# Thema: return_sequences\n",
    "# from https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "\n",
    "# 32 = batch_size\n",
    "# 10 = 10 time steps\n",
    "# 8  = 8 features\n",
    "inputs = np.random.random([32, 10, 8]).astype(np.float32)\n",
    "print(f\"inputs: {inputs.shape}\")\n",
    "lstm = tf.keras.layers.LSTM(4)\n",
    "\n",
    "output = lstm(inputs)  # The output has shape `[32, 4]`.\n",
    "print(f\"output: {output.shape}\")\n",
    "\n",
    "lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)\n",
    "\n",
    "# whole_sequence_output has shape `[32, 10, 4]`.\n",
    "# final_memory_state and final_carry_state both have shape `[32, 4]`.\n",
    "whole_sequence_output, final_memory_state, final_carry_state = lstm(inputs)\n",
    "print(f\"whole_sequence_output: {whole_sequence_output.shape}, final_memory_state: {final_memory_state.shape}, final_carry_state: {final_carry_state.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic: LSTM return_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm cell: <keras.layers.recurrent.LSTM object at 0x000001F50BABA160>\n",
      "lstm1: Tensor(\"lstm_14_1/strided_slice_18:0\", shape=(None, 4), dtype=float32)\n",
      "output.shape: (1, 4)\n",
      "output = [[0.02765957 0.02765957 0.02765957 0.02765957]]\n"
     ]
    }
   ],
   "source": [
    "# Thema: return_sequences\n",
    "# https://www.dlology.com/blog/how-to-use-return_state-or-return_sequences-in-keras/\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "import keras\n",
    "\n",
    "k_init = keras.initializers.Constant(value=0.1)\n",
    "b_init = keras.initializers.Constant(value=0)\n",
    "r_init = keras.initializers.Constant(value=0.1)\n",
    "# LSTM units\n",
    "units = 4\n",
    "\n",
    "# define model\n",
    "inputs1 = Input(shape=(3, 2))\n",
    "\n",
    "lstm1 = LSTM(units, return_sequences=False, kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init)\n",
    "print(f\"lstm cell: {lstm1}\")\n",
    "lstm1 = lstm1(inputs1)\n",
    "print(f\"lstm1: {lstm1}\")\n",
    "model = Model(inputs=inputs1, outputs=lstm1)\n",
    "\n",
    "# define input data\n",
    "data = array([0.1, 0.2, 0.3, 0.1, 0.2, 0.3]).reshape((1,3,2))\n",
    "# make and show prediction\n",
    "output = model.predict(data)\n",
    "print(f\"output.shape: {output.shape}\\noutput = {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Series Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see https://www.tensorflow.org/guide/data#time_series_windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[20 21 22 23 24 25 26 27 28 29]\n",
      "[30 31 32 33 34 35 36 37 38 39]\n"
     ]
    }
   ],
   "source": [
    "range_ds = tf.data.Dataset.range(45)\n",
    "batches = range_ds.batch(10, drop_remainder=True)\n",
    "\n",
    "for batch in batches.take(5):\n",
    "  print(batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_1_step(batch, shift=1):\n",
    "  # Shift features and labels one step relative to each other.\n",
    "  return batch[:-shift], batch[shift:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8]  =>  [1 2 3 4 5 6 7 8 9]\n",
      "[10 11 12 13 14 15 16 17 18]  =>  [11 12 13 14 15 16 17 18 19]\n",
      "[20 21 22 23 24 25 26 27 28]  =>  [21 22 23 24 25 26 27 28 29]\n"
     ]
    }
   ],
   "source": [
    "predict_dense_1_step = batches.map(dense_1_step)\n",
    "\n",
    "for features, label in predict_dense_1_step.take(3):\n",
    "  print(features.numpy(), \" => \", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To predict a whole window instead of a fixed offset you can split the batches into two parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]  =>  [10 11 12 13 14]\n",
      "[15 16 17 18 19 20 21 22 23 24]  =>  [25 26 27 28 29]\n",
      "[30 31 32 33 34 35 36 37 38 39]  =>  [40 41 42 43 44]\n"
     ]
    }
   ],
   "source": [
    "batches = range_ds.batch(15, drop_remainder=True)\n",
    "\n",
    "def label_next_5_steps(batch):\n",
    "  return (batch[:-5],   # Take all but the 5 last steps\n",
    "          batch[-5:])   # take the remainder (5 steps)\n",
    "\n",
    "predict_5_steps = batches.map(label_next_5_steps)\n",
    "\n",
    "for features, label in predict_5_steps.take(3):\n",
    "  print(features.numpy(), \" => \", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To allow some overlap between the features of one batch and the labels of another, use Dataset.zip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]  =>  [10 11 12 13 14]\n",
      "[10 11 12 13 14 15 16 17 18 19]  =>  [20 21 22 23 24]\n",
      "[20 21 22 23 24 25 26 27 28 29]  =>  [30 31 32 33 34]\n"
     ]
    }
   ],
   "source": [
    "feature_length = 10\n",
    "label_length = 5\n",
    "\n",
    "features = range_ds.batch(feature_length, drop_remainder=True)\n",
    "\n",
    "labels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:-5])\n",
    "\n",
    "predict_5_steps = tf.data.Dataset.zip((features, labels))\n",
    "\n",
    "for features, label in predict_5_steps.take(3):\n",
    "  print(features.numpy(), \" => \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_VariantDataset shapes: (), types: tf.int64>\n",
      "<_VariantDataset shapes: (), types: tf.int64>\n",
      "<_VariantDataset shapes: (), types: tf.int64>\n",
      "<_VariantDataset shapes: (), types: tf.int64>\n",
      "<_VariantDataset shapes: (), types: tf.int64>\n"
     ]
    }
   ],
   "source": [
    "window_size = 5\n",
    "\n",
    "windows = range_ds.window(window_size, shift=1)\n",
    "for sub_ds in windows.take(5):\n",
    "  print(sub_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x000001F50BCFD488> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for x in windows.flat_map(lambda x: x).take(30):\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x000001F50BCFD488> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for x in windows.flat_map(lambda x: x).take(30):\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "0 1 2 3 4 1 2 3 4 5 2 3 4 5 6 3 4 5 6 7 4 5 6 7 8 5 6 7 8 9 "
     ]
    }
   ],
   "source": [
    " for x in windows.flat_map(lambda x: x).take(30):\n",
    "   print(x.numpy(), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[1 2 3 4 5]\n",
      "[2 3 4 5 6]\n",
      "[3 4 5 6 7]\n",
      "[4 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "def sub_to_batch(sub):\n",
    "  return sub.batch(window_size, drop_remainder=True)\n",
    "\n",
    "for example in windows.flat_map(sub_to_batch).take(5):\n",
    "  print(example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_window_dataset(ds, window_size=5, shift=1, stride=1):\n",
    "  windows = ds.window(window_size, shift=shift, stride=stride)\n",
    "\n",
    "  def sub_to_batch(sub):\n",
    "    return sub.batch(window_size, drop_remainder=True)\n",
    "\n",
    "  windows = windows.flat_map(sub_to_batch)\n",
    "  return windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[ 2  3  4  5  6  7  8  9 10 11]\n",
      "[ 4  5  6  7  8  9 10 11 12 13]\n",
      "[ 6  7  8  9 10 11 12 13 14 15]\n",
      "[ 8  9 10 11 12 13 14 15 16 17]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[12 13 14 15 16 17 18 19 20 21]\n",
      "[14 15 16 17 18 19 20 21 22 23]\n",
      "[16 17 18 19 20 21 22 23 24 25]\n",
      "[18 19 20 21 22 23 24 25 26 27]\n"
     ]
    }
   ],
   "source": [
    "ds = make_window_dataset(range_ds, window_size=10, shift = 2, stride=1)\n",
    "\n",
    "for example in ds.take(10):\n",
    "  print(example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7] => [2 3 4 5 6 7 8 9]\n",
      "[2 3 4 5 6 7 8 9] => [ 4  5  6  7  8  9 10 11]\n",
      "[ 4  5  6  7  8  9 10 11] => [ 6  7  8  9 10 11 12 13]\n"
     ]
    }
   ],
   "source": [
    "# extract labels, as before:\n",
    "dense_labels_ds = ds.map(lambda x: dense_1_step(x, shift=2))\n",
    "\n",
    "for inputs,labels in dense_labels_ds.take(3):\n",
    "  print(inputs.numpy(), \"=>\", labels.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
